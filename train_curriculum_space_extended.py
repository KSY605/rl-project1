import gymnasium as gym
from gymnasium import spaces
import numpy as np
import stable_baselines3
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv
from stable_baselines3.common.callbacks import BaseCallback
from stable_baselines3.common.utils import get_device
from collections import deque
import xml.etree.ElementTree as ET
import os
import tempfile
import argparse
import torch as th
import zipfile
# --- ì˜¤ë¥˜ ìˆ˜ì •ì„ ìœ„í•´ ì§ì ‘ í´ë˜ìŠ¤ë¥¼ ì„í¬íŠ¸ ---
from gymnasium.envs.mujoco.walker2d_v5 import Walker2dEnv

# --- 0. TensorBoard ë¡œê¹…ì„ ìœ„í•œ ì»¤ìŠ¤í…€ ì½œë°± ---
class TensorboardLoggingCallback(BaseCallback):
    """
    ì—í”¼ì†Œë“œ ì¢…ë£Œ ì‹œ ì»¤ìŠ¤í…€ ì •ë³´ë¥¼ TensorBoardì— ë¡œê¹…í•˜ëŠ” ì½œë°±.
    """
    def __init__(self, verbose=0):
        super(TensorboardLoggingCallback, self).__init__(verbose)

    def _on_step(self) -> bool:
        for i, done in enumerate(self.locals['dones']):
            if done:
                info = self.locals['infos'][i]
                if 'passed_bumps_count' in info:
                    self.logger.record('custom/passed_bumps_count', info['passed_bumps_count'])
                if 'total_bumps_in_level' in info:
                    self.logger.record('custom/total_bumps_in_level', info['total_bumps_in_level'])
        return True

# --- 1. ì»¤ë¦¬í˜ëŸ¼ ê´€ë¦¬ì (Curriculum Manager) ---
class CurriculumManager:
    def __init__(self, bumps_xml_path, level_grouping_thresholds):
        self.all_bumps = self._parse_bumps(bumps_xml_path)
        self.levels = self._create_levels(level_grouping_thresholds)
        self.current_level = 0
        print(f"ì´ {len(self.levels)}ê°œì˜ ì»¤ë¦¬í˜ëŸ¼ ë ˆë²¨ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
        for i, level_bumps in enumerate(self.levels):
            print(f"  - ë ˆë²¨ {i}: ì¥ì• ë¬¼ {len(level_bumps)}ê°œ")

    def _parse_bumps(self, xml_path):
        """XML íŒŒì¼ì—ì„œ ì¥ì• ë¬¼ ì •ë³´ë¥¼ íŒŒì‹±í•˜ê³  x ìœ„ì¹˜ ìˆœìœ¼ë¡œ ì •ë ¬í•©ë‹ˆë‹¤."""
        tree = ET.parse(xml_path)
        root = tree.getroot()
        bumps = []
        for geom in root.findall(".//geom"):
            name = geom.get('name')
            if name and name.startswith('bump'):
                pos = [float(p) for p in geom.get('pos').split()]
                size = [float(s) for s in geom.get('size').split()]
                # XMLì—ì„œ ë†’ì´ëŠ” sizeì˜ zê°’(ì„¸ ë²ˆì§¸ ìš”ì†Œ)ì…ë‹ˆë‹¤.
                bumps.append({'name': name, 'pos_x': pos[0], 'height': size[2], 'xml_element': geom})
        return sorted(bumps, key=lambda b: b['pos_x'])

    def _create_levels(self, thresholds):
        """
        ì ì§„ì ìœ¼ë¡œ ì–´ë ¤ì›Œì§€ëŠ” ì¥ì• ë¬¼ ì»¤ë¦¬í˜ëŸ¼ì„ ìƒì„±í•˜ë©°,
        ìµœì¢… ë ˆë²¨ì´ ê³ ìœ í•˜ë„ë¡ ë³´ì¥í•©ë‹ˆë‹¤.
        """
        all_bumps_sorted_by_height = sorted(self.all_bumps, key=lambda b: b['height'])
        
        levels = []
        # ë ˆë²¨ 0: ì¥ì• ë¬¼ ì—†ìŒ. ê¸°ë³¸ì ì¸ ë³´í–‰ì„ í•™ìŠµí•˜ê¸° ìœ„í•œ ì¢‹ì€ ì‹œì‘ì ì…ë‹ˆë‹¤.
        levels.append([])
        
        # ì¤‘ê°„ ë ˆë²¨: ê° ë†’ì´ ì„ê³„ê°’ê¹Œì§€ì˜ ì¥ì• ë¬¼ì„ í¬í•¨í•©ë‹ˆë‹¤.
        for threshold in sorted(thresholds):
            level_bumps = [b for b in all_bumps_sorted_by_height if b['height'] <= threshold]
            if level_bumps: # ì„ê³„ê°’ì´ ë„ˆë¬´ ë‚®ì•„ ë¹ˆ ë¦¬ìŠ¤íŠ¸ê°€ ì¶”ê°€ë˜ëŠ” ê²ƒì„ ë°©ì§€í•©ë‹ˆë‹¤.
                levels.append(level_bumps)
                
        # ìµœì¢… ë ˆë²¨: ëª¨ë“  ì¥ì• ë¬¼ì˜ ì „ì²´ ì§‘í•©ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
        levels.append(self.all_bumps)
        
        # ìƒì„±ë  ìˆ˜ ìˆëŠ” ì¤‘ë³µ ë ˆë²¨ì„ ì œê±°í•©ë‹ˆë‹¤.
        unique_levels = []
        seen_levels_signatures = set()
        for level in levels:
            # ì¥ì• ë¬¼ ì´ë¦„ì„ ê¸°ë°˜ìœ¼ë¡œ ë ˆë²¨ì˜ ê³ ìœ í•œ "ì‹œê·¸ë‹ˆì²˜"ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
            level_signature = tuple(sorted([b['name'] for b in level]))
            if level_signature not in seen_levels_signatures:
                unique_levels.append(level)
                seen_levels_signatures.add(level_signature)
                
        return unique_levels

    def get_current_level_bumps(self):
        return self.levels[self.current_level]

    def promote(self):
        if self.current_level < len(self.levels) - 1:
            self.current_level += 1
            print(f"\nğŸ‰ğŸ‰ğŸ‰ ì¶•í•˜í•©ë‹ˆë‹¤! ë ˆë²¨ {self.current_level}ë¡œ ìŠ¹ê¸‰í–ˆìŠµë‹ˆë‹¤! ğŸ‰ğŸ‰ğŸ‰")
            print(f"ì´ì œ {len(self.get_current_level_bumps())}ê°œì˜ ì¥ì• ë¬¼ì— ë„ì „í•©ë‹ˆë‹¤.")
            return True
        else:
            print("\nğŸ† ëª¨ë“  ì»¤ë¦¬í˜ëŸ¼ì„ ë§ˆìŠ¤í„°í–ˆìŠµë‹ˆë‹¤! ìµœì¢… í›ˆë ¨ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
            return False

# --- 2. ì»¤ìŠ¤í…€ ì›Œì»¤ í™˜ê²½ (Custom Walker Environment) ---
class CurriculumWalkerEnv(gym.Wrapper):
    def __init__(self, base_xml_path, bumps_for_level, render_mode=None):
        self.base_xml_path = base_xml_path
        self.bumps_for_level = sorted(bumps_for_level, key=lambda b: b['pos_x'])
        self.temp_xml_path = self._create_temp_xml_with_bumps()
        
        env = Walker2dEnv(
            xml_file=self.temp_xml_path, 
            render_mode=render_mode,
            frame_skip=10, 
            healthy_z_range=(0.5, 10.0),
            exclude_current_positions_from_observation=False
        )
        super().__init__(env)
        
        self.passed_bumps = set()
        
        # ì—ì´ì „íŠ¸ê°€ ë‹¤ìŒ Nê°œì˜ ì¥ì• ë¬¼ì„ ë³´ë„ë¡ ê´€ì°° ê³µê°„ì„ í™•ì¥í•©ë‹ˆë‹¤.
        self.num_bumps_to_observe = 3 # ê´€ì°°í•  ì¥ì• ë¬¼ ìˆ˜
        self.num_bump_features = 2 # ì¥ì• ë¬¼ ë‹¹ íŠ¹ì§• ìˆ˜ (ê±°ë¦¬, ë†’ì´)
        base_obs_space = self.env.observation_space
        
        # ì¶”ê°€ë  ì¥ì• ë¬¼ ê´€ì°° ê³µê°„ ì •ì˜
        bump_obs_low = np.full(self.num_bumps_to_observe * self.num_bump_features, -np.inf)
        bump_obs_high = np.full(self.num_bumps_to_observe * self.num_bump_features, np.inf)
        
        # ê¸°ì¡´ ê´€ì°° ê³µê°„ê³¼ ì¥ì• ë¬¼ ê´€ì°° ê³µê°„ì„ í•©ì¹©ë‹ˆë‹¤.
        new_low = np.concatenate([base_obs_space.low, bump_obs_low])
        new_high = np.concatenate([base_obs_space.high, bump_obs_high])
        self.observation_space = spaces.Box(low=new_low, high=new_high, dtype=np.float64)
        
    def _create_temp_xml_with_bumps(self):
        """í˜„ì¬ ë ˆë²¨ì— ë§ëŠ” ì¥ì• ë¬¼ë§Œ í¬í•¨í•˜ëŠ” ì„ì‹œ XML íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        tree = ET.parse(self.base_xml_path)
        root = tree.getroot()
        worldbody = root.find('worldbody')
        
        # ê¸°ì¡´ì˜ ëª¨ë“  'bump' ì§€ì˜¤ë©”íŠ¸ë¦¬ë¥¼ ì œê±°í•©ë‹ˆë‹¤.
        bumps_to_remove = [geom for geom in worldbody.findall(".//geom") if geom.get('name', '').startswith('bump')]
        for geom in bumps_to_remove:
            worldbody.remove(geom)

        # í˜„ì¬ ë ˆë²¨ì— í•´ë‹¹í•˜ëŠ” ì¥ì• ë¬¼ë§Œ ì¶”ê°€í•©ë‹ˆë‹¤.
        for bump_info in self.bumps_for_level:
            worldbody.append(bump_info['xml_element'])
            
        # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
        temp_dir = tempfile.gettempdir()
        # í”„ë¡œì„¸ìŠ¤ IDë¥¼ í¬í•¨í•˜ì—¬ ë³‘ë ¬ ì‹¤í–‰ ì‹œ íŒŒì¼ ì¶©ëŒì„ ë°©ì§€í•©ë‹ˆë‹¤.
        temp_path = os.path.join(temp_dir, f"walker_curriculum_{os.getpid()}.xml")
        tree.write(temp_path)
        return temp_path

    def _get_bump_observation(self):
        """ì—ì´ì „íŠ¸ì˜ í˜„ì¬ ìœ„ì¹˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‹¤ê°€ì˜¤ëŠ” ì¥ì• ë¬¼ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        agent_x = self.unwrapped.data.qpos[0]
        # ì•„ì§ ì§€ë‚˜ê°€ì§€ ì•Šì€ ì¥ì• ë¬¼ ëª©ë¡
        upcoming_bumps = [b for b in self.bumps_for_level if b['pos_x'] > agent_x]
        
        bump_features = []
        for i in range(self.num_bumps_to_observe):
            if i < len(upcoming_bumps):
                bump = upcoming_bumps[i]
                # (ì—ì´ì „íŠ¸ë¡œë¶€í„°ì˜ ìƒëŒ€ ê±°ë¦¬, ì¥ì• ë¬¼ ë†’ì´)
                bump_features.extend([bump['pos_x'] - agent_x, bump['height']])
            else:
                # ê´€ì°°í•  ì¥ì• ë¬¼ì´ ë” ì´ìƒ ì—†ìœ¼ë©´ ë¨¼ ê±°ë¦¬ì™€ 0 ë†’ì´ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ê°’ì„ ì±„ì›ë‹ˆë‹¤.
                bump_features.extend([100.0, 0.0]) # í° ê°’ìœ¼ë¡œ ì¥ì• ë¬¼ì´ ì—†ìŒì„ í‘œì‹œ
        return np.array(bump_features, dtype=np.float32)

    def _get_full_observation(self, base_obs):
        """ê¸°ë³¸ ê´€ì°°ê°’ê³¼ ì¥ì• ë¬¼ ê´€ì°°ê°’ì„ ê²°í•©í•©ë‹ˆë‹¤."""
        return np.concatenate([base_obs, self._get_bump_observation()])

    def step(self, action):
        base_obs, reward, terminated, truncated, info = self.env.step(action)
        x_pos = self.unwrapped.data.qpos[0]
        
        # í†µê³¼í•œ ì¥ì• ë¬¼ ê¸°ë¡
        for bump in self.bumps_for_level:
            if x_pos > bump['pos_x']:
                self.passed_bumps.add(bump['name'])
                
        info['passed_bumps_count'] = len(self.passed_bumps)
        info['total_bumps_in_level'] = len(self.bumps_for_level)
        
        # í˜„ì¬ ë ˆë²¨ì˜ ëª¨ë“  ì¥ì• ë¬¼ì„ í†µê³¼í–ˆëŠ”ì§€ í™•ì¸
        if info['total_bumps_in_level'] > 0 and info['passed_bumps_count'] == info['total_bumps_in_level']:
            info['cleared_all_bumps'] = True
            reward += 100  # í° ë³´ìƒ
            terminated = True # ì—í”¼ì†Œë“œ ì„±ê³µì ìœ¼ë¡œ ì¢…ë£Œ
        else:
            info['cleared_all_bumps'] = False
            
        return self._get_full_observation(base_obs), reward, terminated, truncated, info

    def reset(self, **kwargs):
        self.passed_bumps.clear()
        base_obs, info = self.env.reset(**kwargs)
        info['passed_bumps_count'] = 0
        info['total_bumps_in_level'] = len(self.bumps_for_level)
        return self._get_full_observation(base_obs), info
        
    def close(self):
        super().close()
        # í™˜ê²½ì´ ë‹«í ë•Œ ì„ì‹œ XML íŒŒì¼ì„ ì‚­ì œí•©ë‹ˆë‹¤.
        if hasattr(self, 'temp_xml_path') and os.path.exists(self.temp_xml_path):
            os.remove(self.temp_xml_path)

# --- 3. ê°€ì¤‘ì¹˜ ì´ì‹ í•¨ìˆ˜ ---
def transfer_weights(pretrained_params, new_model_params):
    """Observation Spaceê°€ ë‹¤ë¥¸ ëª¨ë¸ ê°„ ê°€ì¤‘ì¹˜ë¥¼ ì´ì‹í•©ë‹ˆë‹¤."""
    for (new_name, new_param), (old_name, old_param) in zip(new_model_params.items(), pretrained_params.items()):
        if new_param.shape == old_param.shape:
            new_param.data.copy_(old_param.data)
        # ì…ë ¥ ë ˆì´ì–´(ì²« ë²ˆì§¸ ë ˆì´ì–´)ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ë¶„ì ìœ¼ë¡œ ë³µì‚¬
        elif 'policy_net.0.weight' in new_name or 'value_net.0.weight' in new_name:
            if len(new_param.shape) > 1 and len(old_param.shape) > 1:
                old_obs_dim = old_param.shape[1]
                # ìƒˆë¡œìš´ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ í…ì„œì—ì„œ, ê¸°ì¡´ observationì— í•´ë‹¹í•˜ëŠ” ë¶€ë¶„ë§Œ ë³µì‚¬
                new_param.data[:, :old_obs_dim] = old_param.data
                print(f"ì…ë ¥ ë ˆì´ì–´ ê°€ì¤‘ì¹˜ ë¶€ë¶„ ë³µì‚¬ ì™„ë£Œ: {new_name}")
        else:
            print(f"ë ˆì´ì–´ í¬ê¸° ë¶ˆì¼ì¹˜ë¡œ ê±´ë„ˆëœ€: {new_name}. New: {new_param.shape}, Old: {old_param.shape}")
    return new_model_params

# --- 4. ë©”ì¸ í›ˆë ¨ ë£¨í”„ ---
def main(args):
    # --- ì„¤ì • ---
    XML_PATH = 'custom_walker2d_bumps_v2.xml'
    LEVEL_THRESHOLDS = [0.15, 0.3, 0.5, 0.8, 1.3] 
    TOTAL_TIMESTEPS = 2_000_000
    EVAL_EPISODES = 100
    PROMOTION_THRESHOLD = 0.95
    LEARNING_STEPS_PER_EVAL = 50_000
    
    # --- ì´ˆê¸°í™” ---
    CHECKPOINT_DIR = "checkpoints"
    os.makedirs(CHECKPOINT_DIR, exist_ok=True)

    curriculum_manager = CurriculumManager(XML_PATH, LEVEL_THRESHOLDS)
    callback = TensorboardLoggingCallback()

    def make_env(rank=0, seed=0):
        def _init():
            bumps = curriculum_manager.get_current_level_bumps()
            env = CurriculumWalkerEnv(XML_PATH, bumps)
            env.reset(seed=seed + rank)
            return env
        return _init

    vec_env = SubprocVecEnv([make_env(rank=i) for i in range(args.n_envs)])
    
    policy_kwargs = {"net_arch": {"pi": [128, 64, 64], "vf": [128, 64, 64]}}
    
    # --- ëª¨ë¸ ìƒì„± ë˜ëŠ” ë¡œë“œ ---
    if args.pretrained_model and os.path.exists(args.pretrained_model):
        print(f"ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ '{args.pretrained_model}'ì—ì„œ ê°€ì¤‘ì¹˜ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.")
        # ë¨¼ì € ìƒˆ í™˜ê²½ì— ë§ëŠ” ëª¨ë¸ êµ¬ì¡°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        model = PPO("MlpPolicy", vec_env, verbose=0, tensorboard_log="./walker_tensorboard/", policy_kwargs=policy_kwargs)
        
        # ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë§Œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
        with zipfile.ZipFile(args.pretrained_model, "r") as archive:
            # policy.pth íŒŒì¼ ì°¾ê¸°
            policy_file_path = next((name for name in archive.namelist() if name.endswith("policy.pth")), None)
            if policy_file_path is None:
                raise FileNotFoundError(f"'{args.pretrained_model}' zip íŒŒì¼ì—ì„œ policy.pth íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

            with archive.open(policy_file_path, "r") as policy_file:
                pretrained_params = th.load(policy_file, map_location=get_device("auto"))

        new_model_params = model.policy.state_dict()
        # ê°€ì¤‘ì¹˜ ì´ì‹
        transfer_weights(pretrained_params, new_model_params)
        model.policy.load_state_dict(new_model_params)
        print("ê°€ì¤‘ì¹˜ ì´ì‹ ì™„ë£Œ.")
    else:
        print("ìƒˆë¡œìš´ PPO ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤.")
        model = PPO("MlpPolicy", vec_env, verbose=0, tensorboard_log="./walker_tensorboard/", policy_kwargs=policy_kwargs)

    # --- í›ˆë ¨ ì‹œì‘ ---
    timesteps_done = 0
    while timesteps_done < TOTAL_TIMESTEPS:
        # reset_num_timesteps=Falseë¡œ ì„¤ì •í•˜ì—¬ ì´ íƒ€ì„ìŠ¤í…ì´ ëˆ„ì ë˜ë„ë¡ í•©ë‹ˆë‹¤.
        model.learn(total_timesteps=LEARNING_STEPS_PER_EVAL, reset_num_timesteps=False, 
                    tb_log_name=f"PPO_Level_{curriculum_manager.current_level}", callback=callback)
        timesteps_done += LEARNING_STEPS_PER_EVAL
        print(f"\n--- ì´ ì§„í–‰ë¥ : {timesteps_done}/{TOTAL_TIMESTEPS} íƒ€ì„ìŠ¤í… ---")
        print(f"í˜„ì¬ ë ˆë²¨ {curriculum_manager.current_level}ì—ì„œ ì„±ëŠ¥ í‰ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")

        # í‰ê°€ìš© í™˜ê²½ ìƒì„±
        eval_env = DummyVecEnv([make_env()])
        successful_episodes = 0
        for _ in range(EVAL_EPISODES):
            obs = eval_env.reset()
            done = False
            while not done:
                action, _ = model.predict(obs, deterministic=True)
                # VecEnvì˜ step ë©”ì†Œë“œëŠ” 4ê°œì˜ ê°’ì„ ë°˜í™˜í•©ë‹ˆë‹¤: (obs, rewards, dones, infos)
                obs, rewards, dones, infos = eval_env.step(action)
                
                # VecEnvëŠ” ì—¬ëŸ¬ í™˜ê²½ì„ ë‹¤ë£¨ë¯€ë¡œ ê²°ê³¼ëŠ” ë°°ì—´/ë¦¬ìŠ¤íŠ¸ í˜•íƒœì…ë‹ˆë‹¤.
                # ì—¬ê¸°ì„œëŠ” í™˜ê²½ì´ í•˜ë‚˜ì´ë¯€ë¡œ ì²« ë²ˆì§¸ ìš”ì†Œì— ì ‘ê·¼í•©ë‹ˆë‹¤.
                done = dones[0]
                info = infos[0]

                # --- ğŸ”´ ì—ëŸ¬ ìˆ˜ì • ğŸ”´ ---
                # ì—í”¼ì†Œë“œê°€ ëë‚¬ì„ ë•Œ ì„±ê³µ ì—¬ë¶€ë¥¼ íŒë‹¨í•©ë‹ˆë‹¤.
                if done:
                    # ë ˆë²¨ 0 (ì¥ì• ë¬¼ ì—†ìŒ)ì˜ ê²½ìš°, ì—í”¼ì†Œë“œê°€ ëë‚˜ë©´(ë„˜ì–´ì§€ì§€ ì•Šìœ¼ë©´) ì„±ê³µìœ¼ë¡œ ê°„ì£¼í•©ë‹ˆë‹¤.
                    if info.get('total_bumps_in_level', 0) == 0:
                        successful_episodes += 1
                        break # while ë£¨í”„ íƒˆì¶œ
                    # ì¥ì• ë¬¼ì´ ìˆëŠ” ë ˆë²¨ì˜ ê²½ìš°, ëª¨ë“  ì¥ì• ë¬¼ì„ í†µê³¼í•´ì•¼ ì„±ê³µì…ë‹ˆë‹¤.
                    elif info.get('cleared_all_bumps', False):
                        successful_episodes += 1
                        # breakëŠ” í•„ìš” ì—†ìŠµë‹ˆë‹¤. cleared_all_bumpsê°€ Trueì´ë©´ ì–´ì°¨í”¼ terminated=Trueê°€ ë˜ì–´ doneì´ Trueê°€ ë©ë‹ˆë‹¤.

        eval_env.close()
        
        success_rate = successful_episodes / EVAL_EPISODES
        print(f"í‰ê°€ ì™„ë£Œ: ì„±ê³µë¥  {success_rate:.2%} ({successful_episodes}/{EVAL_EPISODES})")

        # ì„±ê³µë¥ ì´ ì„ê³„ê°’ì„ ë„˜ìœ¼ë©´ ë‹¤ìŒ ë ˆë²¨ë¡œ ì§„í–‰
        if success_rate >= PROMOTION_THRESHOLD:
            checkpoint_path = os.path.join(CHECKPOINT_DIR, f"walker_level_{curriculum_manager.current_level}_ec_best.zip")
            model.save(checkpoint_path)
            print(f"âœ… ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì™„ë£Œ: {checkpoint_path}")

            # ë‹¤ìŒ ë ˆë²¨ë¡œ ìŠ¹ê¸‰. ë” ì´ìƒ ë ˆë²¨ì´ ì—†ìœ¼ë©´ ë£¨í”„ ì¢…ë£Œ
            if not curriculum_manager.promote():
                break 
            
            # ë‹¤ìŒ ë ˆë²¨ì„ ìœ„í•´ í™˜ê²½ì„ ìƒˆë¡œ ë§Œë“­ë‹ˆë‹¤.
            vec_env.close()
            vec_env = SubprocVecEnv([make_env(rank=i) for i in range(args.n_envs)])
            model.set_env(vec_env)
        else:
            print(f"ì„±ê³µë¥ ì´ ëª©í‘œ({PROMOTION_THRESHOLD:.0%})ì— ë„ë‹¬í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. í˜„ì¬ ë ˆë²¨ì—ì„œ í›ˆë ¨ì„ ê³„ì†í•©ë‹ˆë‹¤.")

    print("\n--- ìµœì¢… í›ˆë ¨ ë‹¨ê³„ ---")
    # ë‚¨ì€ íƒ€ì„ìŠ¤í…ë§Œí¼ ìµœì¢… í›ˆë ¨ì„ ì§„í–‰í•©ë‹ˆë‹¤.
    model.learn(total_timesteps=max(0, TOTAL_TIMESTEPS - timesteps_done), reset_num_timesteps=False, 
                tb_log_name="PPO_Final", callback=callback)
    
    model.save("walker_curriculum_final.zip")
    print("ìµœì¢… ëª¨ë¸ì´ 'walker_curriculum_final.zip'ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    vec_env.close()

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="Curriculum Learning for Walker2D with Bumps")
    parser.add_argument("--pretrained-model", type=str, default=None, help="ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì˜ ê²½ë¡œ (ì˜ˆ: ./walker.zip)")
    parser.add_argument("--n-envs", type=int, default=4, help="ë³‘ë ¬ë¡œ ì‹¤í–‰í•  í™˜ê²½ì˜ ìˆ˜ (CPU ì½”ì–´ ìˆ˜ì— ë§ê²Œ ì¡°ì ˆ)")
    cli_args = parser.parse_args()

    if not os.path.exists('custom_walker2d_bumps_v2.xml'):
        print("ì—ëŸ¬: 'custom_walker2d_bumps_v2.xml' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    else:
        main(cli_args)
